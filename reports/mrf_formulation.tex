\documentclass[11pt,a4paper,oneside]{report}

\begin{document}
\tableofcontents
\section{Model Formulation 1}
For standard pair-wise Markov Network, we have 
\begin{equation}
  P_\Phi (y) = \frac{1}{Z} \prod_{i=1}^{N} \Phi_i(y_i) \prod_{(i,j)\in E} \Phi_{ij}(y_i,y_j)
  \end{equation}

Modeling $y_i^k$ as a continuous variable(higher value indicates higher confidence that segment i is of class k), we can define the log of the node potential as 
$ log \Phi_i(k) = (y_i^k -  w_n^k.x_i )^2$  and the log of the edge potential as $ log \Phi_{ij}(l,k) = w_e^{l,k}x_{ij} ( y_i^l -  y_j^k)^2$

Substituting these potential functions, we can write the log likelihood as: 

\begin{eqnarray*}
P_w (y|x) &=& \frac{1}{Z}exp(-\sum_{i=1}^{N} \sum_{k=1}^{K} (y_i^k - w^{k}_{n}.x_{i})^2 - \sum_{(i,j)\in E} \sum_{l,k=1}^{K} w_{e}^{l,k}.x_{ij}(y_i^l - y_j^k)^2), where\\
Z&=&\int_{-\infty}^{+\infty }{exp \bigg( - \sum_{i=1}^{N} \sum_{k=1}^{K} (y_i^k - w^{k}_{n}.x_{i})^2 - \sum_{(i,j)\in E} \sum_{l,k=1}^{K} w_{e}^{l,k}.x_{ij}(y_i^l - y_j^k)^2 \bigg) } d^{NK}y\\
&=&\int_{-\infty}^{+\infty }{exp \bigg(-y^T B y+a^T y-\frac{a^Ta}{4}\bigg) } d^{NK}y\\
&=&exp(-\frac{a^Ta}{4})\int_{-\infty}^{+\infty }{exp \bigg(-y^T B y+a^T y\bigg) } d^{NK}y\\
&=&exp(-\frac{a^Ta}{4})\frac{(\sqrt{\pi})^{NK}}{\sqrt{det(B)}}exp(\frac{1}{4}a^T B^{-1} a)\\
&=&exp\bigg(\frac{1}{4}a^T (B^{-1}-I) a+\frac{NK}{2}log(\pi)-\frac{log(det(B))}{2}\bigg)
\end{eqnarray*}
The second last-step uses a formula from wikipedia. \\
Sanity check: On removing the edge potentials, B=I and hence Z becomes independent of a(a depends on $w_n$).
\subsection{Learning}
In learning, we will find the maximum likelihood estimates of $w_n$ and $w_e$.
\begin{eqnarray*}
-log P_w (y|x) &=& \sum_{i=1}^{N} \sum_{k=1}^{K} (y_i^k - w^{k}_{n}.x_{i})^2 + \sum_{(i,j)\in E} \sum_{l,k=1}^{K} w_{e}^{l,k}.x_{ij}(y_i^l - y_j^k)^2) +log(Z)\\
&=& y^T B y-a^T y +\frac{a^Ta}{4} +log(Z)\\
&=& y^T B y-a^T y +\frac{a^Ta}{4}+\frac{1}{4}a^T (B^{-1} -I)a-\frac{log(det(B))}{2}+\frac{NK}{2}log(\pi)\\
&=& y^T B y-a^T y +\frac{1}{4}a^T B^{-1}a-\frac{log(det(B))}{2}+\frac{NK}{2}log(\pi)
\end{eqnarray*}
Note that B is a function of $w_e$ and a is a function of $w_n$.

So, we get:
\begin{eqnarray}
w=\arg\min_{w}:y^T B y-ay^T +\frac{1}{4}a^T B^{-1} a -\frac{log(det(B))}{2}
\end{eqnarray}
This function is not convex in B and a. We could still consider using iterative methods.

\begin{itemize}
\item If we fix $w_e$ such that B becomes a constant positive semi-definite matrix , the the problem becomes convex in a($w_n$). Note that the inverse of a +ve semi-definite matrix (if exists) is always +ve semi-definite(proved in appendix). $w_e^{l,k}x_{ij} \ge 0 \forall i,j,l,k \Rightarrow$ B is positive semidefinite.
Setting derivative w.r.t $a$ to 0,
\begin{eqnarray}
-y^T +\frac{1}{2}a^TB^{-1}=0\\
\Rightarrow (B^{-T} *a =2*y\\
\Rightarrow a=2*B^{T}*y\\
\Rightarrow 2*Xw_n=2*B^{T}*y\\
\Rightarrow w_n=(X^TX)^{-1}X^T*B^{T}*y
\end{eqnarray}


\item if we fix $w_n$, the problem does not seem to be convex in B ($w_e$). We could start by using simulated annealing for this step. We can initialize $w_e$ with values such that $i^{th}$ element of $w_e^{l,k}$ is proportional to the number of times objects l and k occur in configuration i. This makes sure that the value of $w_e^{l,k}x_{i,j}$ is greater when i and j are of classes l and k respectively and hence penalty is high if  $(y_i^l==1 \&\& y_j^k==0) || (y_i^l==0 \&\& y_j^k==1)$
\end{itemize}

\subsection{Inference}
Inference boils down to a QP in y.

\section{Model Formulation 2}
Given labels $\{1,.., K\}$, $y_i^k$ is an indicator variable which is 1 if segment i has label k. 

For standard pair-wise Markov Network, we have 

\begin{equation}
  P_\Phi (y) = \frac{1}{Z} \prod_{i=1}^{N} \Phi_i(y_i) \prod_{(i,j)\in E} \Phi_{ij}(y_i,y_j)
  \end{equation}
  
  We can define the log of node potential as a linear function of the node features:  $log \Phi_i(k) = w_n^k.x_i$ , 
where $x_i$ is the feature vector of node i. Intuitively, the training process(margin-maximization) will try to pick $w_n^1...w_n^K$ such that $w_n^k.x_i>w_n^l.x_i \forall l$ if the correct label is k.\\
  Similarly, the log of edge potential can be defined as $log \Phi_{ij}(l,k) = w_{e}^{l,k}.x_{ij}$. The intuition is that $w_{e}^{l,k}$ will be high if segments of label k and label l appear frequently in configurations having relative features(relations) $x_{ij}$ . $x_{ij}$ could be a vector of indicator variables each indicating whether a particular relation holds between segments i and j.
However, instead that hard-coding the thresholds for relations like on-top, nearby, coplanar , we believe it would be better to include more informative continuous features in $x_{i,j}$ like difference of coordinates of centroids of the 2 segments, difference between mean normals etc.\\

Substituting these potential functions, we can write the log likelihood as: 

\begin{equation}
 \log P_w (y|x) = \sum_{i=1}^{N} \sum_{k=1}^{K} (w^{k}_{n}.x_{i})y_{i}^{k} + \sum_{(i,j)\in E} \sum_{l,k=1}^{K} (w_{e}^{l,k}.x_{ij})y_i^l y_j^k   -   \log Z_w(x)
\end{equation}

Doing some matrix manipulations(as shown in \cite{taskar2004learning}), we get:

\begin{equation}
 \log P_w (y|x) = wXy -logZ_w(x)
\end{equation}
where $w=[w_n w_e], y_n=[y_1^1...y_1^K....y_N^1...y_N^K],y_{i,j}^{l,k}=y_i^l*y_j^k,y_e=[ .....y_{i,j}^{1,1},y_{i,j}^{1,2}...y_{i,j}^{K,K} ....], y=[y_n,y_e]$ and X is constructed accordingly.
$w$ is of size $K*|x_i|+K^2*|x_{ij}|$

\subsection{Learning}
The idea is to maximize the margin by which probability of correct label-sequence beats the probability of any incorrect label-sequence.

\begin{eqnarray*}
min: \frac{1}{2} ||w||^2 + C\epsilon\\
s.t.: wX(\hat{y}-y)\ge \Delta(\hat{y}-y) -\epsilon \forall y\neq \hat{y}
\end{eqnarray*}
where, $\Delta(\hat{y}-y)$ counts the number of elements in which $\hat{y}$ and y differ.
The number of constraints is $K^N-1$. We hope that the cutting plane methods\cite{joachims2009cutting} 
which start by removing all constraints and  iteratively keeps adding most violated constraints would work.
\subsection{Inference}
Inference can be done in 2 ways. However, none of them have any optimality guarantees.

\subsubsection{LP rounding}
Prediction is an integer programming problem(which is NP complete):


\begin{eqnarray*}
y^*=\arg \max _y wXy\\
st: y_{i,j}^{l,k}\le y_i^l\\
y_{i,j}^{l,k}\le y_j^k\\
\forall i \sum_{j=1}^{K} y_i^j = 1
\end{eqnarray*}
 
We have to make sure that the coefficients of $y_{i,j}^{l,k}$ are positive in  y wXy.
% Or used the formulation using marginalization.

We hope we can prove that the LP-relaxation of this problem has provable approximation bounds.

\subsubsection{Graph Cuts}
Consider the log probability formula in original problem (ignoring terms not depending on y):
\begin{equation}
Energy(y)= -\log P_w (y|x) = -\sum_{i=1}^{N} \sum_{k=1}^{K} (w^{k}_{n}.x_{i})y_{i}^{k} - \sum_{(i,j)\in E} \sum_{l,k=1}^{K} (w_{e}^{l,k}.x_{ij})y_i^l y_j^k
\end{equation}
It is easy to see that this function is regular(submodular) if $w_e^{l,k}x_{ij} \ge 0 \forall i,j,l,k$ . Enforcing this constraint does not seem to cause any loss in generality. For objects that usually co-occur in a configuration ,learning algorithm can set $w_e^{l,k}x_{ij}$ higher so as to minimize energy when they occur together during inference. So, this energy can be minimized {\bf exactly} by using graph-cuts(the regularity constraints make sure that the corresponding graph has no negative edges). The only problem is that the optimal solution might have multiple indicators true for the same segment or none of the indicators true for a segment. We could add terms of the form $\infty y_i^ky_i^l \forall i, l\neq k$ but that breaks regularity.
Hopefully, the number of segments for which this happens is small and hopefully those segments form many disconnected components. In this case, an exhaustive search can be done for labels of only these segments.
If exhaustive search cannot be done, one heuristic could  be using the solution of LP rounding  to break the tie.

\section*{Appendix}
It's easy to see that inverse of a +ve semidefinite invertible matrix is semi-definite.
From the diagonalized representation, it is clear that a matrix is +ve semidefinite if and only if all it's eigen-values are positive.
\begin{eqnarray*}
Ax=\lambda x\\
A^{-1}Ax=\lambda A^{-1}x\\
\frac{1}{\lambda}x=A^{-1}x
\end{eqnarray*}

\bibliographystyle{alpha}
\bibliography{mrf_formulation}
\end{document}
